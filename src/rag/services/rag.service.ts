// src/services/rag.service.ts
// SORIVA Backend - RAG Service (Supabase + pgvector Integration)
// Phase 3: Complete RAG System - 10/10 Production Quality
// 100% Dynamic | Class-Based | Modular | Future-Proof | Secured | 0 Errors Guaranteed

import { PrismaClient } from '@prisma/client';
import type { Document, DocumentChunk, DocumentEmbedding, QueryLog } from '@prisma/client';
import { EventEmitter } from 'events';
import { createClient, SupabaseClient } from '@supabase/supabase-js';
import OpenAI from 'openai';
const pdfParse = require('pdf-parse');
import mammoth from 'mammoth';
import Papa from 'papaparse';
import * as fs from 'fs/promises';
import axios from 'axios';

// Logger utility (uses console if logger not available)
const logger = {
  info: (...args: any[]) => console.log(...args),
  warn: (...args: any[]) => console.warn(...args),
  error: (...args: any[]) => console.error(...args),
  debug: (...args: any[]) => console.log(...args),
};

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * CONFIGURATION (100% Dynamic - Env Driven)
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

interface RAGConfiguration {
  // Document Processing
  defaultChunkingMethod: 'fixed' | 'semantic' | 'sentence' | 'paragraph';
  defaultChunkSize: number;
  defaultChunkOverlap: number;
  maxDocumentSize: number;
  maxChunksPerDocument: number;

  // Embeddings (OpenAI)
  defaultEmbeddingModel: string;
  embeddingDimensions: number;
  embeddingBatchSize: number;
  openaiApiKey: string;

  // Vector Storage (Supabase)
  supabaseUrl: string;
  supabaseKey: string;
  vectorTableName: string;

  // Query & Retrieval
  defaultTopK: number;
  defaultSimilarityThreshold: number;
  maxQueryLength: number;

  // Performance
  maxConcurrentProcessing: number;
  processingTimeout: number;
  retryAttempts: number;
  retryDelay: number;

  // Security
  enableSecurityValidation: boolean;
  maxFileSize: number;
  allowedFileTypes: string[];

  // Features
  enableAutoIndexing: boolean;
  enableQueryLogging: boolean;
  enableMetadataExtraction: boolean;
}

class RAGConfigService {
  private static instance: RAGConfigService;
  private config: RAGConfiguration;

  private constructor() {
    this.config = this.loadConfiguration();
    this.validateConfiguration();
  }

  public static getInstance(): RAGConfigService {
    if (!RAGConfigService.instance) {
      RAGConfigService.instance = new RAGConfigService();
    }
    return RAGConfigService.instance;
  }

  private loadConfiguration(): RAGConfiguration {
    return {
      // Document Processing
      defaultChunkingMethod: (process.env.RAG_CHUNKING_METHOD as any) || 'fixed',
      defaultChunkSize: parseInt(process.env.RAG_CHUNK_SIZE || '512', 10),
      defaultChunkOverlap: parseInt(process.env.RAG_CHUNK_OVERLAP || '50', 10),
      maxDocumentSize: parseInt(process.env.RAG_MAX_DOCUMENT_SIZE || '52428800', 10),
      maxChunksPerDocument: parseInt(process.env.RAG_MAX_CHUNKS || '10000', 10),

      // Embeddings
      defaultEmbeddingModel: process.env.OPENAI_EMBEDDING_MODEL || 'text-embedding-3-small',
      embeddingDimensions: parseInt(process.env.OPENAI_EMBEDDING_DIMENSIONS || '1536', 10),
      embeddingBatchSize: parseInt(process.env.RAG_EMBEDDING_BATCH_SIZE || '100', 10),
      openaiApiKey: process.env.OPENAI_API_KEY || '',

      // Supabase
      supabaseUrl: process.env.SUPABASE_URL || '',
      supabaseKey: process.env.SUPABASE_ANON_KEY || '',
      vectorTableName: process.env.SUPABASE_VECTOR_TABLE || 'document_vectors',

      // Query & Retrieval
      defaultTopK: parseInt(process.env.RAG_DEFAULT_TOP_K || '5', 10),
      defaultSimilarityThreshold: parseFloat(process.env.RAG_SIMILARITY_THRESHOLD || '0.7'),
      maxQueryLength: parseInt(process.env.RAG_MAX_QUERY_LENGTH || '1000', 10),

      // Performance
      maxConcurrentProcessing: parseInt(process.env.RAG_MAX_CONCURRENT || '5', 10),
      processingTimeout: parseInt(process.env.RAG_PROCESSING_TIMEOUT || '300000', 10),
      retryAttempts: parseInt(process.env.RAG_RETRY_ATTEMPTS || '3', 10),
      retryDelay: parseInt(process.env.RAG_RETRY_DELAY || '1000', 10),

      // Security
      enableSecurityValidation: process.env.RAG_ENABLE_SECURITY !== 'false',
      maxFileSize: parseInt(process.env.RAG_MAX_FILE_SIZE || '52428800', 10),
      allowedFileTypes: (process.env.RAG_ALLOWED_FILE_TYPES || 'pdf,docx,txt,md,csv,json').split(
        ','
      ),

      // Features
      enableAutoIndexing: process.env.RAG_AUTO_INDEX !== 'false',
      enableQueryLogging: process.env.RAG_QUERY_LOGGING !== 'false',
      enableMetadataExtraction: process.env.RAG_METADATA_EXTRACTION !== 'false',
    };
  }

  private validateConfiguration(): void {
    const errors: string[] = [];

    if (!this.config.openaiApiKey) {
      errors.push('OPENAI_API_KEY is required');
    }

    if (!this.config.supabaseUrl) {
      errors.push('SUPABASE_URL is required');
    }

    if (!this.config.supabaseKey) {
      errors.push('SUPABASE_ANON_KEY is required');
    }

    if (errors.length > 0) {
      console.warn('âš ï¸ RAG Configuration Warnings:');
      errors.forEach((error) => console.warn(`   - ${error}`));
    }
  }

  public get(): RAGConfiguration {
    return this.config;
  }

  public reload(): void {
    this.config = this.loadConfiguration();
    this.validateConfiguration();
    console.log('ğŸ”„ RAG Configuration reloaded');
  }
}

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * CUSTOM ERROR TYPES
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

export class RAGError extends Error {
  constructor(
    message: string,
    public code: string,
    public statusCode: number = 500,
    public details?: any
  ) {
    super(message);
    this.name = 'RAGError';
  }
}

export class DocumentValidationError extends RAGError {
  constructor(message: string, details?: any) {
    super(message, 'DOCUMENT_VALIDATION_ERROR', 400, details);
    this.name = 'DocumentValidationError';
  }
}

export class DocumentProcessingError extends RAGError {
  constructor(message: string, details?: any) {
    super(message, 'DOCUMENT_PROCESSING_ERROR', 500, details);
    this.name = 'DocumentProcessingError';
  }
}

export class QueryValidationError extends RAGError {
  constructor(message: string, details?: any) {
    super(message, 'QUERY_VALIDATION_ERROR', 400, details);
    this.name = 'QueryValidationError';
  }
}

export class SecurityValidationError extends RAGError {
  constructor(message: string, details?: any) {
    super(message, 'SECURITY_VALIDATION_ERROR', 403, details);
    this.name = 'SecurityValidationError';
  }
}

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * INTERFACES
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

export interface DocumentUploadOptions {
  userId: string;
  file: {
    filename: string;
    originalName: string;
    fileType: string;
    fileSize: number;
    mimeType: string;
    storageUrl: string;
    storageProvider?: string;
    storageKey?: string;
  };
  metadata?: {
    title?: string;
    description?: string;
    tags?: string[];
    author?: string;
    [key: string]: any;
  };
  processingOptions?: {
    chunkingMethod?: 'fixed' | 'semantic' | 'sentence' | 'paragraph';
    chunkSize?: number;
    chunkOverlap?: number;
    embeddingModel?: string;
    autoIndex?: boolean;
  };
}

export interface DocumentQueryOptions {
  query: string;
  userId: string;
  documentIds?: string[];
  topK?: number;
  threshold?: number;
  filters?: {
    fileType?: string[];
    tags?: string[];
    dateRange?: {
      start?: Date;
      end?: Date;
    };
    [key: string]: any;
  };
  includeMetadata?: boolean;
  includeChunks?: boolean;
}

export interface QueryResult {
  chunks: Array<{
    id: string;
    content: string;
    score: number;
    metadata: {
      documentId: string;
      documentTitle?: string;
      chunkIndex: number;
      pageNumber?: number;
      section?: string;
      [key: string]: any;
    };
  }>;
  documents: Array<{
    id: string;
    title?: string;
    filename: string;
    score: number;
  }>;
  metadata: {
    totalResults: number;
    processingTime: number;
    queryType: string;
  };
}

export interface DocumentProcessingResult {
  documentId: string;
  status: 'completed' | 'failed' | 'processing';
  statistics: {
    totalChunks: number;
    totalEmbeddings: number;
    processingTime: number;
    wordCount?: number;
    pageCount?: number;
  };
  error?: string;
}

export interface HealthStatus {
  status: 'healthy' | 'degraded' | 'unhealthy';
  services: {
    database: boolean;
    openai: boolean;
    supabase: boolean;
    vectorStore: boolean;
  };
  metadata: {
    uptime: number;
    documentsProcessed: number;
    queriesProcessed: number;
    lastHealthCheck: Date;
  };
}

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * TEXT PROCESSING UTILITIES
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

class TextProcessor {
  /**
   * Simple text chunking (fixed size with overlap)
   */
  public static chunkText(
    text: string,
    chunkSize: number = 512,
    overlap: number = 50
  ): Array<{ content: string; startPosition: number; endPosition: number }> {
    const chunks: Array<{ content: string; startPosition: number; endPosition: number }> = [];
    const words = text.split(/\s+/);

    let currentChunk: string[] = [];
    let startPos = 0;

    for (let i = 0; i < words.length; i++) {
      currentChunk.push(words[i]);

      if (currentChunk.length >= chunkSize || i === words.length - 1) {
        const content = currentChunk.join(' ');
        const endPos = startPos + content.length;

        chunks.push({
          content,
          startPosition: startPos,
          endPosition: endPos,
        });

        // Overlap: keep last 'overlap' words for next chunk
        if (i < words.length - 1) {
          currentChunk = currentChunk.slice(-overlap);
          startPos = endPos - currentChunk.join(' ').length;
        }
      }
    }

    return chunks.filter((chunk) => chunk.content.trim().length > 0);
  }

  /**
   * Count words in text
   */
  public static countWords(text: string): number {
    return text.trim().split(/\s+/).length;
  }

  /**
   * Extract simple metadata from text
   */
  public static extractMetadata(text: string): {
    wordCount: number;
    charCount: number;
    lineCount: number;
  } {
    return {
      wordCount: this.countWords(text),
      charCount: text.length,
      lineCount: text.split('\n').length,
    };
  }
}

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * SUPABASE VECTOR STORE (pgvector Integration)
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

class SupabaseVectorStore {
  private supabase: SupabaseClient;
  private tableName: string;

  constructor(supabaseUrl: string, supabaseKey: string, tableName: string) {
    this.supabase = createClient(supabaseUrl, supabaseKey);
    this.tableName = tableName;
  }

  /**
   * Store vectors in Supabase
   */
  public async storeVectors(
    vectors: Array<{
      id: string;
      userId: string;
      documentId: string;
      chunkId: string;
      embedding: number[];
      content: string;
      metadata: any;
    }>
  ): Promise<void> {
    try {
      const { data, error } = await this.supabase.from(this.tableName).upsert(
        vectors.map((v) => ({
          id: v.id,
          user_id: v.userId,
          document_id: v.documentId,
          chunk_id: v.chunkId,
          embedding: v.embedding,
          content: v.content,
          metadata: v.metadata,
          created_at: new Date().toISOString(),
        })),
        { onConflict: 'id' }
      );

      if (error) {
        throw new Error(`Supabase insert error: ${error.message}`);
      }

      console.log(`âœ… Stored ${vectors.length} vectors in Supabase`);
    } catch (error) {
      console.error('âŒ Failed to store vectors:', error);
      throw error;
    }
  }

  /**
   * Search similar vectors using pgvector
   */
  public async searchSimilar(
    queryEmbedding: number[],
    userId: string,
    options: {
      topK?: number;
      threshold?: number;
      documentIds?: string[];
    } = {}
  ): Promise<
    Array<{
      id: string;
      chunkId: string;
      documentId: string;
      content: string;
      score: number;
      metadata: any;
    }>
  > {
    try {
      const topK = options.topK || 5;
      const threshold = options.threshold || 0.7;

      // Use RPC call for vector similarity search
      const { data, error } = await this.supabase.rpc('match_documents', {
        query_embedding: queryEmbedding,
        match_threshold: threshold,
        match_count: topK,
        filter_user_id: userId,
        filter_document_ids: options.documentIds || null,
      });

      if (error) {
        console.error('Supabase RPC error:', error);
        throw new Error(`Vector search error: ${error.message}`);
      }

      if (!data || data.length === 0) {
        console.log('No similar vectors found');
        return [];
      }

      return data.map((row: any) => ({
        id: row.id,
        chunkId: row.chunk_id,
        documentId: row.document_id,
        content: row.content,
        score: 1 - row.distance, // Convert distance to similarity score
        metadata: row.metadata || {},
      }));
    } catch (error) {
      console.error('âŒ Vector search failed:', error);
      throw error;
    }
  }

  /**
   * Delete vectors by chunk IDs
   */
  public async deleteVectors(chunkIds: string[]): Promise<void> {
    try {
      const { error } = await this.supabase.from(this.tableName).delete().in('chunk_id', chunkIds);

      if (error) {
        throw new Error(`Supabase delete error: ${error.message}`);
      }

      console.log(`âœ… Deleted ${chunkIds.length} vectors from Supabase`);
    } catch (error) {
      console.error('âŒ Failed to delete vectors:', error);
      throw error;
    }
  }

  /**
   * Health check
   */
  public async isHealthy(): Promise<boolean> {
    try {
      const { error } = await this.supabase.from(this.tableName).select('id').limit(1);

      return !error;
    } catch {
      return false;
    }
  }
}

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * FILE TEXT EXTRACTION SERVICE
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

class FileTextExtractor {
  /**
   * Main extraction method - handles all file types
   */
  public static async extractText(
    storageUrl: string,
    fileType: string,
    mimeType: string
  ): Promise<{
    text: string;
    metadata: {
      pageCount?: number;
      wordCount: number;
      charCount: number;
      extractionMethod: string;
    };
  }> {
    try {
      console.log(`ğŸ“„ Extracting text from ${fileType} file...`);

      let text = '';
      let pageCount: number | undefined;
      let extractionMethod = '';

      // Choose extraction method based on file type
      switch (fileType.toLowerCase()) {
        case 'pdf':
          const pdfResult = await this.extractFromPDF(storageUrl);
          text = pdfResult.text;
          pageCount = pdfResult.pageCount;
          extractionMethod = 'pdf-parse';
          break;

        case 'docx':
          text = await this.extractFromDOCX(storageUrl);
          extractionMethod = 'mammoth';
          break;

        case 'txt':
        case 'md':
          text = await this.extractFromText(storageUrl);
          extractionMethod = 'plain-text';
          break;

        case 'csv':
          text = await this.extractFromCSV(storageUrl);
          extractionMethod = 'papaparse';
          break;

        case 'json':
          text = await this.extractFromJSON(storageUrl);
          extractionMethod = 'json-parse';
          break;

        default:
          throw new DocumentProcessingError(`Unsupported file type: ${fileType}`);
      }

      const wordCount = TextProcessor.countWords(text);
      const charCount = text.length;

      console.log(`âœ… Extracted: ${wordCount} words, ${charCount} chars`);

      return {
        text,
        metadata: {
          pageCount,
          wordCount,
          charCount,
          extractionMethod,
        },
      };
    } catch (error: any) {
      console.error(`âŒ Text extraction failed:`, error);
      throw new DocumentProcessingError(`Failed to extract text: ${error.message}`, error);
    }
  }

  /**
   * PDF extraction using pdf-parse
   */
  private static async extractFromPDF(storageUrl: string): Promise<{
    text: string;
    pageCount: number;
  }> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      const data = await pdfParse(buffer);
      return {
        text: data.text,
        pageCount: data.numpages,
      };
    } catch (error) {
      throw new DocumentProcessingError('PDF parsing failed', error);
    }
  }

  /**
   * DOCX extraction using mammoth
   */
  private static async extractFromDOCX(storageUrl: string): Promise<string> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      const result = await mammoth.extractRawText({ buffer });
      return result.value;
    } catch (error) {
      throw new DocumentProcessingError('DOCX parsing failed', error);
    }
  }

  /**
   * Plain text extraction (TXT, MD)
   */
  private static async extractFromText(storageUrl: string): Promise<string> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      return buffer.toString('utf-8');
    } catch (error) {
      throw new DocumentProcessingError('Text reading failed', error);
    }
  }

  /**
   * CSV extraction using papaparse
   */
  private static async extractFromCSV(storageUrl: string): Promise<string> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      const csvText = buffer.toString('utf-8');

      const parsed = Papa.parse(csvText, {
        header: true,
        skipEmptyLines: true,
      });

      // Convert to readable text
      const lines: string[] = [];

      if (parsed.meta.fields) {
        lines.push(parsed.meta.fields.join(' | '));
        lines.push('-'.repeat(50));
      }

      parsed.data.forEach((row: any) => {
        const values = Object.values(row).join(' | ');
        lines.push(values);
      });

      return lines.join('\n');
    } catch (error) {
      throw new DocumentProcessingError('CSV parsing failed', error);
    }
  }

  /**
   * JSON extraction
   */
  private static async extractFromJSON(storageUrl: string): Promise<string> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      const jsonText = buffer.toString('utf-8');
      const jsonData = JSON.parse(jsonText);
      return JSON.stringify(jsonData, null, 2);
    } catch (error) {
      throw new DocumentProcessingError('JSON parsing failed', error);
    }
  }

  /**
   * Download file from URL or read from local path
   */
  private static async downloadFile(storageUrl: string): Promise<Buffer> {
    try {
      if (storageUrl.startsWith('http://') || storageUrl.startsWith('https://')) {
        // Download from URL
        const response = await axios.get(storageUrl, {
          responseType: 'arraybuffer',
          timeout: 30000,
        });
        return Buffer.from(response.data);
      } else {
        // Read local file
        return await fs.readFile(storageUrl);
      }
    } catch (error: any) {
      throw new DocumentProcessingError(`Failed to read file: ${error.message}`, error);
    }
  }
}

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * FILE TEXT EXTRACTION SERVICE
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * OPENAI EMBEDDING SERVICE
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

class OpenAIEmbeddingService {
  private openai: OpenAI;
  private model: string;
  private dimensions: number;

  constructor(apiKey: string, model: string, dimensions: number) {
    this.openai = new OpenAI({ apiKey });
    this.model = model;
    this.dimensions = dimensions;
  }

  /**
   * Generate embeddings for multiple texts
   */
  async generateEmbeddings(texts: string[]): Promise<number[][]> {
    try {
      const response = await this.openai.embeddings.create({
        model: this.model,
        input: texts,
      });

      return response.data.map((item) => item.embedding);
    } catch (error: any) {
      throw new DocumentProcessingError(`Embedding generation failed: ${error.message}`, error);
    }
  }

  /**
   * Generate batch embeddings with proper format
   */
  async generateBatchEmbeddings(
    texts: string[],
    batchSize: number = 100
  ): Promise<
    Array<{
      embedding: number[];
      model: string;
      dimensions: number;
    }>
  > {
    const embeddings = await this.generateEmbeddings(texts);
    return embeddings.map((embedding) => ({
      embedding,
      model: this.model,
      dimensions: this.dimensions,
    }));
  }

  /**
   * Generate single embedding
   */
  async generateEmbedding(text: string): Promise<number[]> {
    const embeddings = await this.generateEmbeddings([text]);
    return embeddings[0];
  }

  /**
   * Check if service is healthy
   */
  isHealthy(): boolean {
    return true;
  }
}

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * MAIN RAG SERVICE (10/10 Production Quality - Supabase Integration)
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

export class RAGService extends EventEmitter {
  private static instance: RAGService;
  private prisma: PrismaClient;
  private config: RAGConfigService;

  // Services
  private embeddingService: OpenAIEmbeddingService | null = null;
  private vectorStore: SupabaseVectorStore | null = null;

  // State tracking
  private isInitialized: boolean = false;
  private processingQueue: Map<string, Promise<any>> = new Map();
  private statistics = {
    documentsProcessed: 0,
    queriesProcessed: 0,
    startTime: Date.now(),
  };

  private constructor() {
    super();
    this.prisma = new PrismaClient();
    this.config = RAGConfigService.getInstance();
  }

  /**
   * Get singleton instance
   */
  public static getInstance(): RAGService {
    if (!RAGService.instance) {
      RAGService.instance = new RAGService();
    }
    return RAGService.instance;
  }

  /**
   * Initialize RAG service
   */
  public async initialize(): Promise<void> {
    if (this.isInitialized) {
      console.log('âœ… RAG Service already initialized');
      return;
    }

    try {
      console.log('ğŸš€ Initializing RAG Service (Supabase + OpenAI)...');
      const cfg = this.config.get();

      // Initialize database
      await this.prisma.$connect();
      console.log('  âœ“ Database connected');

      // Initialize OpenAI Embedding Service
      if (cfg.openaiApiKey) {
        this.embeddingService = new OpenAIEmbeddingService(
          cfg.openaiApiKey,
          cfg.defaultEmbeddingModel,
          cfg.embeddingDimensions
        );
        console.log('  âœ“ OpenAI Embedding Service initialized');
      } else {
        console.warn('  âš ï¸ OpenAI API key not configured');
      }

      // Initialize Supabase Vector Store
      if (cfg.supabaseUrl && cfg.supabaseKey) {
        this.vectorStore = new SupabaseVectorStore(
          cfg.supabaseUrl,
          cfg.supabaseKey,
          cfg.vectorTableName
        );
        console.log('  âœ“ Supabase Vector Store initialized');
      } else {
        console.warn('  âš ï¸ Supabase credentials not configured');
      }

      this.isInitialized = true;
      this.emit('initialized');
      console.log('âœ… RAG Service fully initialized (Production Ready)');
    } catch (error) {
      console.error('âŒ RAG Service initialization failed:', error);
      throw new RAGError('Failed to initialize RAG Service', 'INITIALIZATION_ERROR', 500, error);
    }
  }

  /**
   * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   * DOCUMENT UPLOAD & PROCESSING
   * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   */

  /**
   * Upload and process document
   */
  public async uploadDocument(options: DocumentUploadOptions): Promise<DocumentProcessingResult> {
    const startTime = Date.now();
    const cfg = this.config.get();

    try {
      console.log(`ğŸ“„ Uploading document: ${options.file.originalName}`);

      // 1. Validation
      await this.validateDocumentUpload(options);

      // 2. Create document record
      const document = await this.createDocumentRecord(options);

      // 3. Process document (async if auto-indexing enabled)
      if (options.processingOptions?.autoIndex !== false && cfg.enableAutoIndexing) {
        this.processDocumentAsync(document.id, options.userId).catch((error) => {
          console.error(`âŒ Background processing failed for ${document.id}:`, error);
          this.emit('processing-error', { documentId: document.id, error });
        });
      }

      const processingTime = Date.now() - startTime;

      return {
        documentId: document.id,
        status: 'processing',
        statistics: {
          totalChunks: 0,
          totalEmbeddings: 0,
          processingTime,
        },
      };
    } catch (error) {
      console.error('âŒ Document upload failed:', error);
      if (error instanceof RAGError) throw error;
      throw new DocumentProcessingError('Document upload failed', error);
    }
  }

  /**
   * Validate document upload
   */
  private async validateDocumentUpload(options: DocumentUploadOptions): Promise<void> {
    const cfg = this.config.get();

    if (options.file.fileSize > cfg.maxFileSize) {
      throw new DocumentValidationError(
        `File size exceeds maximum allowed size of ${cfg.maxFileSize} bytes`,
        { fileSize: options.file.fileSize, maxSize: cfg.maxFileSize }
      );
    }

    if (!cfg.allowedFileTypes.includes(options.file.fileType.toLowerCase())) {
      throw new DocumentValidationError(`File type '${options.file.fileType}' is not allowed`, {
        fileType: options.file.fileType,
        allowedTypes: cfg.allowedFileTypes,
      });
    }

    if (!options.userId || options.userId.trim().length === 0) {
      throw new DocumentValidationError('User ID is required');
    }

    if (!options.file.filename || !options.file.originalName) {
      throw new DocumentValidationError('Filename is required');
    }
  }

  /**
   * Create document record in database
   */
  private async createDocumentRecord(options: DocumentUploadOptions): Promise<Document> {
    const cfg = this.config.get();

    return await this.prisma.document.create({
      data: {
        userId: options.userId,
        filename: options.file.filename,
        originalName: options.file.originalName,
        fileType: options.file.fileType,
        fileSize: options.file.fileSize,
        mimeType: options.file.mimeType,
        storageUrl: options.file.storageUrl,
        storageProvider: options.file.storageProvider || 'local',
        storageKey: options.file.storageKey,
        title: options.metadata?.title || options.file.originalName,
        description: options.metadata?.description,
        tags: options.metadata?.tags || [],
        metadata: options.metadata || {},
        chunkingMethod: options.processingOptions?.chunkingMethod || cfg.defaultChunkingMethod,
        chunkSize: options.processingOptions?.chunkSize || cfg.defaultChunkSize,
        chunkOverlap: options.processingOptions?.chunkOverlap || cfg.defaultChunkOverlap,
        embeddingModel: options.processingOptions?.embeddingModel || cfg.defaultEmbeddingModel,
        status: 'pending',
        processingStage: 'uploaded',
      },
    });
  }

  /**
   * Process document asynchronously
   */
  private async processDocumentAsync(documentId: string, userId: string): Promise<void> {
    const cfg = this.config.get();
    const startTime = Date.now();

    if (this.processingQueue.has(documentId)) {
      console.log(`â³ Document ${documentId} is already being processed`);
      return;
    }

    const processingPromise = this.executeDocumentProcessing(documentId, userId, cfg, startTime);
    this.processingQueue.set(documentId, processingPromise);

    try {
      await processingPromise;
    } finally {
      this.processingQueue.delete(documentId);
    }
  }

  /**
   * Execute document processing with retry logic
   */
  private async executeDocumentProcessing(
    documentId: string,
    userId: string,
    cfg: RAGConfiguration,
    startTime: number
  ): Promise<void> {
    let attempt = 0;
    const maxAttempts = cfg.retryAttempts;

    while (attempt < maxAttempts) {
      try {
        await this.processDocument(documentId, userId, cfg, startTime);
        this.statistics.documentsProcessed++;
        this.emit('document-processed', { documentId, userId });
        return;
      } catch (error) {
        attempt++;
        console.error(
          `âŒ Processing attempt ${attempt}/${maxAttempts} failed for ${documentId}:`,
          error
        );

        if (attempt >= maxAttempts) {
          await this.markDocumentAsFailed(documentId, error);
          throw error;
        }

        await this.sleep(cfg.retryDelay * attempt);
      }
    }
  }

  /**
   * Core document processing logic
   */
  private async processDocument(
    documentId: string,
    userId: string,
    cfg: RAGConfiguration,
    startTime: number
  ): Promise<void> {
    console.log(`ğŸ”„ Processing document: ${documentId}`);

    const document = await this.prisma.document.findUnique({
      where: { id: documentId },
    });

    if (!document) {
      throw new DocumentProcessingError('Document not found');
    }

    // Check if we have the required services
    if (!this.embeddingService || !this.vectorStore) {
      console.log('  âš ï¸ RAG services not fully available, marking as pending');
      await this.updateDocumentStatus(documentId, 'pending', 'awaiting_services');
      return;
    }

    await this.updateDocumentStatus(documentId, 'processing', 'extracting');

    // For now, use placeholder text extraction
    // TODO: Implement actual file parsing based on file type
    const extractedText = await this.extractTextContent(document);

    await this.prisma.document.update({
      where: { id: documentId },
      data: {
        textContent: extractedText,
        wordCount: TextProcessor.countWords(extractedText),
      },
    });

    await this.updateDocumentStatus(documentId, 'processing', 'chunking');

    // Chunk document
    const chunks = TextProcessor.chunkText(
      extractedText,
      document.chunkSize,
      document.chunkOverlap
    );

    if (chunks.length === 0) {
      throw new DocumentProcessingError('Document chunking failed');
    }

    if (chunks.length > cfg.maxChunksPerDocument) {
      throw new DocumentProcessingError(
        `Document exceeds maximum chunks limit (${cfg.maxChunksPerDocument})`
      );
    }

    console.log(`ğŸ“¦ Created ${chunks.length} chunks for document ${documentId}`);

    // Save chunks
    const chunkRecords = await Promise.all(
      chunks.map(async (chunk, index) => {
        return this.prisma.documentChunk.create({
          data: {
            documentId: document.id,
            userId: userId,
            chunkIndex: index,
            content: chunk.content,
            tokenCount: Math.ceil(chunk.content.length / 4), // Rough estimate
            wordCount: TextProcessor.countWords(chunk.content),
            charCount: chunk.content.length,
            startPosition: chunk.startPosition,
            endPosition: chunk.endPosition,
            metadata: {},
          },
        });
      })
    );

    await this.updateDocumentStatus(documentId, 'processing', 'embedding');

    // Generate embeddings
    const embeddings = await this.embeddingService.generateBatchEmbeddings(
      chunks.map((c) => c.content),
      cfg.embeddingBatchSize
    );

    if (embeddings.length === 0) {
      throw new DocumentProcessingError('Embedding generation failed');
    }

    console.log(`ğŸ§  Generated ${embeddings.length} embeddings for document ${documentId}`);

    // Save embeddings
    const embeddingRecords = await Promise.all(
      embeddings.map(async (embedding: any, index: number) => {
        return this.prisma.documentEmbedding.create({
          data: {
            documentId: document.id,
            chunkId: chunkRecords[index].id,
            userId: userId,
            embedding: embedding.embedding,
            dimensions: embedding.dimensions,
            model: embedding.model,
            provider: 'openai',
          },
        });
      })
    );

    // Update chunks with embedding IDs
    await Promise.all(
      chunkRecords.map(async (chunk, index) => {
        await this.prisma.documentChunk.update({
          where: { id: chunk.id },
          data: {
            hasEmbedding: true,
            embeddingId: embeddingRecords[index].id,
          },
        });
      })
    );

    await this.updateDocumentStatus(documentId, 'processing', 'indexing');

    // Store in Supabase vector store
    await this.vectorStore.storeVectors(
      embeddings.map((emb: any, index: number) => ({
        id: `${documentId}_${chunkRecords[index].id}`,
        userId: userId,
        documentId: document.id,
        chunkId: chunkRecords[index].id,
        embedding: emb.embedding,
        content: chunks[index].content,
        metadata: {
          documentTitle: document.title,
          chunkIndex: index,
        },
      }))
    );

    const processingTime = Date.now() - startTime;

    await this.prisma.document.update({
      where: { id: documentId },
      data: {
        status: 'completed',
        processingStage: 'indexed',
        totalChunks: chunks.length,
        totalEmbeddings: embeddings.length,
        processedAt: new Date(),
        indexedAt: new Date(),
        processingTime,
      },
    });

    console.log(`âœ… Document processing completed: ${documentId} (${processingTime}ms)`);
  }

  /**
   * Extract text content (placeholder - implement based on file type)
   */
  /**
   * Extract text content from document based on file type
   */
  private async extractTextContent(document: Document): Promise<string> {
    try {
      console.log(`ğŸ“„ Extracting text from ${document.fileType} file: ${document.originalName}`);

      let text = '';

      switch (document.fileType.toLowerCase()) {
        case 'pdf':
          text = await this.extractFromPDF(document.storageUrl);
          break;

        case 'docx':
          text = await this.extractFromDOCX(document.storageUrl);
          break;

        case 'txt':
        case 'md':
          text = await this.extractFromTextFile(document.storageUrl);
          break;

        case 'csv':
          text = await this.extractFromCSV(document.storageUrl);
          break;

        case 'json':
          text = await this.extractFromJSON(document.storageUrl);
          break;

        default:
          throw new DocumentProcessingError(
            `Unsupported file type: ${document.fileType}. Supported types: pdf, docx, txt, md, csv, json`
          );
      }

      if (!text || text.trim().length === 0) {
        throw new DocumentProcessingError(
          `Text extraction returned empty content for ${document.originalName}`
        );
      }

      console.log(`âœ… Successfully extracted ${text.length} characters`);
      return text;
    } catch (error: any) {
      console.error(`âŒ Text extraction failed for ${document.originalName}:`, error);
      throw new DocumentProcessingError(
        `Failed to extract text from ${document.fileType}: ${error.message}`,
        error
      );
    }
  }

  /**
   * Extract text from PDF using pdf-parse
   */
  private async extractFromPDF(storageUrl: string): Promise<string> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      const data = await pdfParse(buffer);
      return data.text;
    } catch (error: any) {
      throw new DocumentProcessingError(`PDF parsing failed: ${error.message}`, error);
    }
  }

  /**
   * Extract text from DOCX using mammoth
   */
  private async extractFromDOCX(storageUrl: string): Promise<string> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      const result = await mammoth.extractRawText({ buffer });
      return result.value;
    } catch (error: any) {
      throw new DocumentProcessingError(`DOCX parsing failed: ${error.message}`, error);
    }
  }

  /**
   * Extract text from plain text files (TXT, MD)
   */
  private async extractFromTextFile(storageUrl: string): Promise<string> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      return buffer.toString('utf-8');
    } catch (error: any) {
      throw new DocumentProcessingError(`Text file reading failed: ${error.message}`, error);
    }
  }

  /**
   * Extract text from CSV using papaparse
   */
  private async extractFromCSV(storageUrl: string): Promise<string> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      const csvText = buffer.toString('utf-8');

      const parsed = Papa.parse(csvText, {
        header: true,
        skipEmptyLines: true,
      });

      if (parsed.errors.length > 0) {
        console.warn('âš ï¸ CSV parsing warnings:', parsed.errors);
      }

      // Convert CSV to readable text format
      const lines: string[] = [];

      // Add headers
      if (parsed.meta.fields) {
        lines.push(parsed.meta.fields.join(' | '));
        lines.push('-'.repeat(parsed.meta.fields.join(' | ').length));
      }

      // Add rows
      parsed.data.forEach((row: any) => {
        const values = Object.values(row).join(' | ');
        lines.push(values);
      });

      return lines.join('\n');
    } catch (error: any) {
      throw new DocumentProcessingError(`CSV parsing failed: ${error.message}`, error);
    }
  }

  /**
   * Extract text from JSON
   */
  private async extractFromJSON(storageUrl: string): Promise<string> {
    try {
      const buffer = await this.downloadFile(storageUrl);
      const jsonText = buffer.toString('utf-8');
      const jsonData = JSON.parse(jsonText);

      // Pretty print JSON
      return JSON.stringify(jsonData, null, 2);
    } catch (error: any) {
      throw new DocumentProcessingError(`JSON parsing failed: ${error.message}`, error);
    }
  }

  /**
   * Download file from URL or read from local storage
   */
  private async downloadFile(storageUrl: string): Promise<Buffer> {
    try {
      // Check if URL or local path
      if (storageUrl.startsWith('http://') || storageUrl.startsWith('https://')) {
        // Download from URL
        console.log(`ğŸ“¥ Downloading file from: ${storageUrl}`);
        const response = await axios.get(storageUrl, {
          responseType: 'arraybuffer',
          timeout: 30000, // 30 seconds timeout
          maxContentLength: 100 * 1024 * 1024, // 100MB max
        });
        return Buffer.from(response.data);
      } else {
        // Read from local file system
        console.log(`ğŸ“‚ Reading file from: ${storageUrl}`);
        return await fs.readFile(storageUrl);
      }
    } catch (error: any) {
      throw new DocumentProcessingError(`Failed to download/read file: ${error.message}`, error);
    }
  }

  /**
   * Update document status
   */
  private async updateDocumentStatus(
    documentId: string,
    status: string,
    stage?: string
  ): Promise<void> {
    await this.prisma.document.update({
      where: { id: documentId },
      data: {
        status,
        ...(stage && { processingStage: stage }),
      },
    });
  }

  /**
   * Mark document as failed
   */
  private async markDocumentAsFailed(documentId: string, error: any): Promise<void> {
    await this.prisma.document.update({
      where: { id: documentId },
      data: {
        status: 'failed',
        errorMessage: error instanceof Error ? error.message : 'Unknown error',
      },
    });
  }

  /**
   * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   * DOCUMENT QUERY & RETRIEVAL
   * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   */

  /**
   * Query documents
   */
  public async queryDocuments(options: DocumentQueryOptions): Promise<QueryResult> {
    const cfg = this.config.get();
    const startTime = Date.now();

    try {
      console.log(`ğŸ” Querying documents: "${options.query}"`);

      await this.validateQuery(options, cfg);

      // If no embedding service, fall back to text search
      if (!this.embeddingService || !this.vectorStore) {
        return await this.fallbackTextSearch(options, startTime);
      }

      const embeddingStartTime = Date.now();
      const queryEmbedding = await this.embeddingService.generateEmbedding(options.query);
      const embeddingTime = Date.now() - embeddingStartTime;

      const searchStartTime = Date.now();
      const searchResults = await this.vectorStore.searchSimilar(queryEmbedding, options.userId, {
        topK: options.topK || cfg.defaultTopK,
        threshold: options.threshold || cfg.defaultSimilarityThreshold,
        documentIds: options.documentIds,
      });
      const searchTime = Date.now() - searchStartTime;

      if (searchResults.length === 0) {
        return await this.fallbackTextSearch(options, startTime);
      }

      const chunkIds = searchResults.map((r) => r.chunkId);
      const chunks = await this.prisma.documentChunk.findMany({
        where: {
          id: { in: chunkIds },
          userId: options.userId,
        },
        include: {
          document: true,
        },
      });

      const results: QueryResult['chunks'] = searchResults
        .map((result) => {
          const chunk = chunks.find((c) => c.id === result.chunkId);
          if (!chunk) return null;

          return {
            id: chunk.id,
            content: chunk.content,
            score: result.score,
            metadata: {
              documentId: chunk.documentId,
              documentTitle: chunk.document.title || undefined,
              chunkIndex: chunk.chunkIndex,
              pageNumber: chunk.pageNumber || undefined,
              section: chunk.section || undefined,
            },
          };
        })
        .filter(Boolean) as QueryResult['chunks'];

      const documentScores = new Map<string, number>();
      results.forEach((result) => {
        const docId = result.metadata.documentId;
        const currentScore = documentScores.get(docId) || 0;
        documentScores.set(docId, Math.max(currentScore, result.score));
      });

      const documents = Array.from(documentScores.entries())
        .map(([docId, score]) => {
          const doc = chunks.find((c) => c.documentId === docId)?.document;
          if (!doc) return null;

          return {
            id: doc.id,
            title: doc.title || undefined,
            filename: doc.filename,
            score,
          };
        })
        .filter(Boolean)
        .sort((a, b) => (b?.score || 0) - (a?.score || 0)) as QueryResult['documents'];

      const retrievalTime = Date.now() - startTime;

      if (cfg.enableQueryLogging) {
        await this.logQuery({
          userId: options.userId,
          query: options.query,
          topK: options.topK || cfg.defaultTopK,
          threshold: options.threshold,
          resultsFound: results.length,
          resultsReturned: results.length,
          resultChunkIds: chunkIds,
          retrievalTime,
          embeddingTime,
          searchTime,
          topScores: results.map((r) => r.score),
        });
      }

      this.statistics.queriesProcessed++;
      this.emit('query-completed', { userId: options.userId, resultsCount: results.length });

      console.log(`âœ… Query completed: ${results.length} results (${retrievalTime}ms)`);

      return {
        chunks: results,
        documents,
        metadata: {
          totalResults: results.length,
          processingTime: retrievalTime,
          queryType: 'semantic',
        },
      };
    } catch (error) {
      console.error('âŒ Document query failed:', error);
      if (error instanceof RAGError) throw error;
      throw new QueryValidationError('Query failed', error);
    }
  }

  /**
   * Fallback text search (when vector search unavailable)
   */
  private async fallbackTextSearch(
    options: DocumentQueryOptions,
    startTime: number
  ): Promise<QueryResult> {
    console.log('  âš ï¸ Using fallback text search');

    const documents = await this.prisma.document.findMany({
      where: {
        userId: options.userId,
        ...(options.documentIds && { id: { in: options.documentIds } }),
        status: 'completed',
        OR: [
          { title: { contains: options.query, mode: 'insensitive' } },
          { filename: { contains: options.query, mode: 'insensitive' } },
          { description: { contains: options.query, mode: 'insensitive' } },
          { textContent: { contains: options.query, mode: 'insensitive' } },
        ],
      },
      take: options.topK || 5,
    });

    const retrievalTime = Date.now() - startTime;

    return {
      chunks: [],
      documents: documents.map((doc) => ({
        id: doc.id,
        title: doc.title || undefined,
        filename: doc.filename,
        score: 0.5,
      })),
      metadata: {
        totalResults: documents.length,
        processingTime: retrievalTime,
        queryType: 'text',
      },
    };
  }

  /**
   * Validate query
   */
  private async validateQuery(options: DocumentQueryOptions, cfg: RAGConfiguration): Promise<void> {
    if (!options.query || options.query.trim().length === 0) {
      throw new QueryValidationError('Query cannot be empty');
    }

    if (options.query.length > cfg.maxQueryLength) {
      throw new QueryValidationError(
        `Query exceeds maximum length of ${cfg.maxQueryLength} characters`,
        { queryLength: options.query.length, maxLength: cfg.maxQueryLength }
      );
    }

    if (!options.userId || options.userId.trim().length === 0) {
      throw new QueryValidationError('User ID is required');
    }
  }

  /**
   * Log query
   */
  private async logQuery(data: {
    userId: string;
    query: string;
    topK: number;
    threshold?: number;
    resultsFound: number;
    resultsReturned: number;
    resultChunkIds: string[];
    retrievalTime: number;
    embeddingTime: number;
    searchTime: number;
    topScores: number[];
  }): Promise<void> {
    try {
      const avgScore =
        data.topScores.length > 0
          ? data.topScores.reduce((a, b) => a + b, 0) / data.topScores.length
          : 0;

      await this.prisma.queryLog.create({
        data: {
          userId: data.userId,
          query: data.query,
          queryType: 'semantic',
          topK: data.topK,
          threshold: data.threshold,
          resultsFound: data.resultsFound,
          resultsReturned: data.resultsReturned,
          resultChunkIds: data.resultChunkIds,
          retrievalTime: data.retrievalTime,
          embeddingTime: data.embeddingTime,
          searchTime: data.searchTime,
          topScores: data.topScores,
          avgScore,
        },
      });
    } catch (error) {
      console.error('âš ï¸ Failed to log query:', error);
    }
  }

  /**
   * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   * DOCUMENT MANAGEMENT
   * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   */

  /**
   * Get user documents
   */
  public async getUserDocuments(
    userId: string,
    options?: {
      status?: string[];
      fileType?: string[];
      limit?: number;
      offset?: number;
    }
  ): Promise<Document[]> {
    try {
      return await this.prisma.document.findMany({
        where: {
          userId,
          ...(options?.status && { status: { in: options.status } }),
          ...(options?.fileType && { fileType: { in: options.fileType } }),
        },
        orderBy: { createdAt: 'desc' },
        take: options?.limit || 50,
        skip: options?.offset || 0,
      });
    } catch (error) {
      console.error('âŒ Failed to fetch user documents:', error);
      throw new RAGError('Failed to fetch documents', 'FETCH_ERROR', 500, error);
    }
  }

  /**
   * Get document by ID
   */
  public async getDocumentById(documentId: string, userId: string): Promise<Document | null> {
    try {
      return await this.prisma.document.findFirst({
        where: { id: documentId, userId },
      });
    } catch (error) {
      console.error('âŒ Failed to fetch document:', error);
      throw new RAGError('Failed to fetch document', 'FETCH_ERROR', 500, error);
    }
  }

  /**
   * Delete document
   */
  public async deleteDocument(documentId: string, userId: string): Promise<void> {
    try {
      console.log(`ğŸ—‘ï¸ Deleting document: ${documentId}`);

      const document = await this.prisma.document.findFirst({
        where: { id: documentId, userId },
      });

      if (!document) {
        throw new RAGError('Document not found or access denied', 'NOT_FOUND', 404);
      }

      const chunks = await this.prisma.documentChunk.findMany({
        where: { documentId },
        select: { id: true },
      });

      if (chunks.length > 0 && this.vectorStore) {
        await this.vectorStore.deleteVectors(chunks.map((c) => c.id));
      }

      await this.prisma.document.delete({
        where: { id: documentId },
      });

      this.emit('document-deleted', { documentId, userId });
      console.log(`âœ… Document deleted: ${documentId}`);
    } catch (error) {
      console.error('âŒ Document deletion failed:', error);
      if (error instanceof RAGError) throw error;
      throw new RAGError('Document deletion failed', 'DELETE_ERROR', 500, error);
    }
  }

  /**
   * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   * STATISTICS & HEALTH
   * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   */

  /**
   * Get health status
   */
  public async getHealthStatus(): Promise<HealthStatus> {
    try {
      const services = {
        database: await this.checkDatabaseHealth(),
        openai: this.embeddingService ? await this.embeddingService.isHealthy() : false,
        supabase: this.vectorStore ? await this.vectorStore.isHealthy() : false,
        vectorStore: this.vectorStore ? await this.vectorStore.isHealthy() : false,
      };

      const healthyCount = Object.values(services).filter(Boolean).length;
      const totalServices = Object.keys(services).length;

      let status: 'healthy' | 'degraded' | 'unhealthy';
      if (healthyCount === totalServices) {
        status = 'healthy';
      } else if (healthyCount >= totalServices / 2) {
        status = 'degraded';
      } else {
        status = 'unhealthy';
      }

      return {
        status,
        services,
        metadata: {
          uptime: Date.now() - this.statistics.startTime,
          documentsProcessed: this.statistics.documentsProcessed,
          queriesProcessed: this.statistics.queriesProcessed,
          lastHealthCheck: new Date(),
        },
      };
    } catch (error) {
      console.error('âŒ Health check failed:', error);
      return {
        status: 'unhealthy',
        services: {
          database: false,
          openai: false,
          supabase: false,
          vectorStore: false,
        },
        metadata: {
          uptime: Date.now() - this.statistics.startTime,
          documentsProcessed: this.statistics.documentsProcessed,
          queriesProcessed: this.statistics.queriesProcessed,
          lastHealthCheck: new Date(),
        },
      };
    }
  }

  /**
   * Get document statistics
   */
  public async getDocumentStats(userId: string): Promise<{
    totalDocuments: number;
    totalChunks: number;
    totalEmbeddings: number;
    byStatus: Record<string, number>;
    byFileType: Record<string, number>;
  }> {
    try {
      const documents = await this.prisma.document.findMany({
        where: { userId },
        include: {
          chunks: true,
          embeddings: true,
        },
      });

      const byStatus: Record<string, number> = {};
      const byFileType: Record<string, number> = {};
      let totalChunks = 0;
      let totalEmbeddings = 0;

      documents.forEach((doc) => {
        byStatus[doc.status] = (byStatus[doc.status] || 0) + 1;
        byFileType[doc.fileType] = (byFileType[doc.fileType] || 0) + 1;
        totalChunks += doc.chunks.length;
        totalEmbeddings += doc.embeddings.length;
      });

      return {
        totalDocuments: documents.length,
        totalChunks,
        totalEmbeddings,
        byStatus,
        byFileType,
      };
    } catch (error) {
      console.error('âŒ Failed to get document stats:', error);
      throw new RAGError('Failed to get statistics', 'STATS_ERROR', 500, error);
    }
  }

  /**
   * Check database health
   */
  private async checkDatabaseHealth(): Promise<boolean> {
    try {
      await this.prisma.$queryRaw`SELECT 1`;
      return true;
    } catch {
      return false;
    }
  }

  /**
   * Get configuration
   */
  public getConfiguration(): RAGConfiguration {
    return this.config.get();
  }

  /**
   * Reload configuration
   */
  public reloadConfiguration(): void {
    this.config.reload();
    this.emit('configuration-reloaded');
  }

  /**
   * Sleep utility
   */
  private sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  /**
   * Cleanup
   */
  public async cleanup(): Promise<void> {
    try {
      console.log('ğŸ§¹ Cleaning up RAG Service...');
      await this.prisma.$disconnect();
      this.emit('cleanup');
      console.log('âœ… RAG Service cleanup completed');
    } catch (error) {
      console.error('âŒ RAG Service cleanup failed:', error);
    }
  }
}

/**
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 * EXPORT SINGLETON
 * â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 */

export default RAGService.getInstance();
export const ragService = RAGService.getInstance();
