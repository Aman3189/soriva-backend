// src/services/gemini-token-counter.service.ts

/**
 * ==========================================
 * GEMINI TOKEN COUNTER SERVICE
 * ==========================================
 * Created by: Amandeep, Punjab, India
 * Purpose: Count input tokens using Gemini Flash (CHEAPEST option!)
 * 
 * SMART COST STRATEGY:
 * - Input tokens: Count with Gemini Flash ($0.10/Million) ‚úÖ
 * - Output tokens: Generated by Smart Routing (Mistral/Claude/GPT)
 * - Pool deduction: Only INPUT tokens (user saves 60%+ tokens!)
 * 
 * WHY GEMINI FOR COUNTING:
 * - Gemini Flash: $0.10/M input tokens
 * - Mistral Large: $2.00/M input tokens  
 * - GPT-5.1: $2.50/M input tokens
 * - Claude Haiku: $0.80/M input tokens
 * 
 * Using Gemini for input processing = 20x cheaper than GPT!
 */

import { GoogleGenerativeAI } from '@google/generative-ai';

// ==========================================
// INTERFACES
// ==========================================

export interface TokenCountResult {
  success: boolean;
  inputTokens: number;
  error?: string;
  model: string;
  costPerMillion: number;
}

export interface MessageForCounting {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

// ==========================================
// GEMINI TOKEN COUNTER CLASS
// ==========================================

class GeminiTokenCounterService {
  private genAI: GoogleGenerativeAI | null = null;
  private model: any = null;
  private isInitialized: boolean = false;

  // Gemini Flash pricing (as of Feb 2026)
  private readonly COST_PER_MILLION = 0.10; // $0.10 per million tokens
  private readonly MODEL_NAME = 'gemini-2.0-flash';

  constructor() {
    this.initialize();
  }

  /**
   * Initialize Gemini client
   */
  private initialize(): void {
    const apiKey = process.env.GEMINI_API_KEY;
    
    if (!apiKey) {
      console.warn('[GeminiTokenCounter] ‚ö†Ô∏è GEMINI_API_KEY not found - falling back to estimation');
      return;
    }

    try {
      this.genAI = new GoogleGenerativeAI(apiKey);
      this.model = this.genAI.getGenerativeModel({ model: this.MODEL_NAME });
      this.isInitialized = true;
      console.log('[GeminiTokenCounter] ‚úÖ Initialized with Gemini Flash');
    } catch (error) {
      console.error('[GeminiTokenCounter] ‚ùå Failed to initialize:', error);
    }
  }

  /**
   * Count tokens for a single message
   */
  async countTokens(text: string): Promise<TokenCountResult> {
    if (!this.isInitialized || !this.model) {
      // Fallback: Estimate ~4 characters per token
      const estimatedTokens = Math.ceil(text.length / 4);
      return {
        success: true,
        inputTokens: estimatedTokens,
        model: 'estimation',
        costPerMillion: this.COST_PER_MILLION,
      };
    }

    try {
      const result = await this.model.countTokens(text);
      
      return {
        success: true,
        inputTokens: result.totalTokens,
        model: this.MODEL_NAME,
        costPerMillion: this.COST_PER_MILLION,
      };
    } catch (error: any) {
      console.error('[GeminiTokenCounter] ‚ùå countTokens failed:', error?.message);
      
      // Fallback to estimation
      const estimatedTokens = Math.ceil(text.length / 4);
      return {
        success: true,
        inputTokens: estimatedTokens,
        error: error?.message,
        model: 'estimation-fallback',
        costPerMillion: this.COST_PER_MILLION,
      };
    }
  }

  /**
   * Count tokens for chat messages (user message + system prompt + history)
   * This is what we'll use for pool deduction
   */
  async countChatInputTokens(
    userMessage: string,
    systemPrompt: string,
    conversationHistory: MessageForCounting[] = []
  ): Promise<TokenCountResult> {
    // Build full input context
    let fullInput = '';

    // Add system prompt
    if (systemPrompt) {
      fullInput += `[SYSTEM]\n${systemPrompt}\n\n`;
    }

    // Add conversation history
    for (const msg of conversationHistory) {
      const roleLabel = msg.role === 'user' ? 'USER' : msg.role === 'assistant' ? 'ASSISTANT' : 'SYSTEM';
      fullInput += `[${roleLabel}]\n${msg.content}\n\n`;
    }

    // Add current user message
    fullInput += `[USER]\n${userMessage}`;

    // Count tokens
    const result = await this.countTokens(fullInput);

    console.log(`[GeminiTokenCounter] üìä Input tokens counted: ${result.inputTokens} | Model: ${result.model}`);

    return result;
  }

  /**
   * Quick estimation without API call (for rate limiting checks)
   */
  estimateTokens(text: string): number {
    // ~4 characters per token (rough estimate)
    return Math.ceil(text.length / 4);
  }

  /**
   * Estimate chat input tokens without API call
   */
  estimateChatInputTokens(
    userMessage: string,
    systemPrompt: string,
    conversationHistory: MessageForCounting[] = []
  ): number {
    let totalChars = systemPrompt.length + userMessage.length;
    
    for (const msg of conversationHistory) {
      totalChars += msg.content.length;
    }

    return Math.ceil(totalChars / 4);
  }

  /**
   * Get service status
   */
  getStatus(): { initialized: boolean; model: string; costPerMillion: number } {
    return {
      initialized: this.isInitialized,
      model: this.isInitialized ? this.MODEL_NAME : 'estimation',
      costPerMillion: this.COST_PER_MILLION,
    };
  }
}

// ==========================================
// EXPORT SINGLETON
// ==========================================

export const geminiTokenCounter = new GeminiTokenCounterService();
export default geminiTokenCounter;